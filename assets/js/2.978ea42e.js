(window.webpackJsonp=window.webpackJsonp||[]).push([[2],{151:function(t,s){t.exports="data:image/png;base64,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"},152:function(t,s,a){t.exports=a.p+"assets/img/fit_curve_data.0ade0d4b.png"},153:function(t,s,a){t.exports=a.p+"assets/img/fit_curve_random.c18f0455.png"},154:function(t,s){t.exports="data:image/png;base64,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"},155:function(t,s,a){t.exports=a.p+"assets/img/fit_curve_learned.2b6e11a0.png"},156:function(t,s){t.exports="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtUlEQVR4AWMYCiDuvztOOYkr321xSs7+W49FVAlMlv15Lo4hpXCoAUw//7sNQ07u6gUREN32+zMPhqTXX7Cpai//NmBauPevLIiK/Pu3EFOy+G8lE1Dji7/bmDAlzT//rZZm6Pv7d3nlXEzZqK9/7x5+8/fv3zUmWLypMgco82CeMSPWoGEUX/F3A+5Q/XNfHqek2t86nHIs1//64JRk+/vZEqdk6t8ruFNA7RdP3JJVLvRJhgCT+UI64+Oa1gAAAABJRU5ErkJggg=="},157:function(t,s){t.exports="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAsklEQVR4AWMYWOB5aM4ca04cku4v3/z9d2eHIyt2aR6/9R///o3AabbEpF9f8Vj96i9uOfdvB3HKSX78m49T3+W/h4WwSbB0vnr17e9Dceweuf/3779FQjjMlJ//7/87TVw2sqnP+/XRFbdPSv9+1MQpyXnkrydurf5/Z2MRVU0UAkv+wyZ58u8RMwYGwSNYQ+jf33taDIZH/j7E5tNdf/++BobQBxdsLuFLB4eQxgAnVgAFLEoKN+TfqAAAAABJRU5ErkJggg=="},158:function(t,s){t.exports="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4ElEQVR4AWMYxEBs/j8g+FjChkVO9/FfIDj87+8kZkzJh3//ztXRYbO6/zcNU/Lz3xWMIFr1eUvFZ38MyaUQxiWg6Tk4JGO+/H3lz4ohuZJRTIMn8vvfd+YYds75+zdX6Orff39vmmA6iG3P329+F/7+W8WDLRDitgJdcraRC3sQqX/+928eP47ws/uy+83fo6LYw/bUcQbF7f8OCWKRY1p5Hmik6de/Olgk9f/GgqgJWCVb/4L9vg275FN5IJn275cWVp2durr+x/6vwOZY479g8EwEmyRzA0hurizt0yMAPzVusb3IFxEAAAAASUVORK5CYII="},159:function(t,s,a){t.exports=a.p+"assets/img/mnist_learned.98c29458.png"},166:function(t,s,a){"use strict";a.r(s);var n=[function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"tensorflow-js"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tensorflow-js","aria-hidden":"true"}},[t._v("#")]),t._v(" TensorFlow.js")]),t._v(" "),n("h2",{attrs:{id:"安装"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#安装","aria-hidden":"true"}},[t._v("#")]),t._v(" 安装")]),t._v(" "),n("p",[t._v("在JavaScript项目中，TensorFlow.js的安装方法有两种：一种是通过script标签引入，另外一种就是通过npm进行安装。")]),t._v(" "),n("p",[t._v("使用Script Tag")]),t._v(" "),n("p",[t._v("将下面的代码添加到HTML文件中，在浏览器中打开该HTML文件，代码应该运行！")]),t._v(" "),n("div",{staticClass:"language-html extra-class"},[n("pre",{pre:!0,attrs:{class:"language-html"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("html")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("head")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- Load TensorFlow.js --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("script")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("src")]),n("span",{pre:!0,attrs:{class:"token attr-value"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')]),t._v("https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.9.0"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')])]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),n("span",{pre:!0,attrs:{class:"token script language-javascript"}},[t._v(" ")]),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("script")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- Place your code in the script tag below. You can also use an external .js file --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("script")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),n("span",{pre:!0,attrs:{class:"token script language-javascript"}},[t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Notice there is no 'import' statement. 'tf' is available on the index-page")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// because of the script tag above.")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Define a model for linear regression.")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sequential")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dense")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("units"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputShape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Prepare the model for training: Specify the loss and the optimizer.")]),t._v("\n      model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'meanSquaredError'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Generate some synthetic data for training.")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" xs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" ys "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Train the model using the data.")]),t._v("\n      model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("then")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Use the model to do inference on a data point the model hasn't seen before:")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Open the browser devtools to see the output")]),t._v("\n        model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    ")]),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("script")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("head")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("body")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("body")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("html")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),n("p",[t._v("通过NPM（或yarn）")]),t._v(" "),n("p",[t._v("使用yarn或npm将TensorFlow.js添加到您的项目中。注意：因为使用ES2017语法（如import），所以此工作流程假定您使用打包程序/转换程序将代码转换为浏览器可以理解的内容。")]),t._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[t._v("yarn add @tensorflow/tfjs\n"),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("npm")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" @tensorflow/tfjs\n")])])]),n("p",[t._v("在js文件中输入以下代码：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'@tensorflow/tfjs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Define a model for linear regression.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sequential")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dense")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("units"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inputShape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Prepare the model for training: Specify the loss and the optimizer.")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'meanSquaredError'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Generate some synthetic data for training.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" xs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" ys "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Train the model using the data.")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("then")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Use the model to do inference on a data point the model hasn't seen before:")]),t._v("\n  model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("h2",{attrs:{id:"核心概念"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#核心概念","aria-hidden":"true"}},[t._v("#")]),t._v(" 核心概念")]),t._v(" "),n("p",[t._v("TensorFlow.js是一个用于机器智能的开源基于WebGL加速的JavaScript库。它将高性能机器学习构建块带到您的指尖，使您能够在浏览器中训练神经网络或在推理模式下运行预先训练的模型。有关安装/配置TensorFlow.js的指南，请参阅Tensorflow.js 安装。")]),t._v(" "),n("p",[t._v("TensorFlow.js为机器学习提供低级构建模块，以及构建神经网络的高级Keras启发式API。我们来看看库的一些核心组件。")]),t._v(" "),n("h3",{attrs:{id:"张量和变量"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#张量和变量","aria-hidden":"true"}},[t._v("#")]),t._v(" 张量和变量")]),t._v(" "),n("p",[t._v("张量(Tensor)和变量（Variable）是TensorFlow.js中数据的主要表现形式，两者不同之处在于张量是不可变的，而变量是可变的。")]),t._v(" "),n("p",[n("strong",[t._v("张量(Tensor)")])]),t._v(" "),n("p",[t._v("TensorFlow.js中数据的中心单位是张量：一组数值形成一个或多个维度的数组。张量实例具有定义数组形状的形状属性。")]),t._v(" "),n("p",[t._v("Tensorflow.js中数据的主要表现形式就是tensor（张量）：由一组数值形成一维或多维数组。一个"),n("code",[t._v("Tensor")]),t._v("实例有一个"),n("code",[t._v("shape")]),t._v("属性来定义这一组数值如何组成张量,而最主要的"),n("code",[t._v("Tensor")]),t._v("实例的构造函数就是"),n("code",[t._v("tf.tensor")]),t._v("函数，如下所示：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2x3 Tensor")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" shape "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2 行, 3 列")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" a "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\na"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 打印张量值")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出:    [[1 , 2 , 3 ],")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//          [10, 20, 30]]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// shape也可以用下面的方式实现:")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" b "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出:    [[1 , 2 , 3 ],")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//          [10, 20, 30]]")]),t._v("\n")])])]),n("p",[t._v("但是，为了构造低秩张量，我们推荐使用下面的函数来增强代码的可读性："),n("code",[t._v("tf.scalar（零维）")]),t._v(", "),n("code",[t._v("tf.tensor1d（一维）")]),t._v(", "),n("code",[t._v("tf.tensor2d（二维）")]),t._v(", "),n("code",[t._v("tf.tensor3d（三维）")]),t._v("、"),n("code",[t._v("tf.tensor4d（四维）")]),t._v("以及"),n("code",[t._v("tf.ones（值全是1）")]),t._v("或者"),n("code",[t._v("tf.zeros（值全是0）")]),t._v(" ，如下所示：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" a "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.14")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\na"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出零维张量")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" b "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出二维张量")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("zeros")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出2行3列的值全是0的张量")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" d "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("ones")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出3行5列的值全是1的张量")]),t._v("\n")])])]),n("p",[t._v("在TensorFlow.js中，张量是不变的; 一旦创建你就不能改变它们的值。 但是，您可以对它们执行操作来生成新的张量。")]),t._v(" "),n("p",[n("strong",[t._v("变量（Variable")])]),t._v(" "),n("p",[t._v("Variables变量是通过张量进行初始化得到的。不像Tensor的值不可变，变量的值是可变的。你可以使用变量的assign方法分配一个新的tensor到这个变量上，这是变量就会改变：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" initialValues "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("zeros")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" biases "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("variable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("initialValues"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 初始化biases")]),t._v("\nbiases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出: [0, 0, 0, 0, 0]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" updatedValues "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor1d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nbiases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("assign")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("updatedValues"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 更新 biases的值")]),t._v("\nbiases"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 输出: [0, 1, 0, 1, 0]")]),t._v("\n")])])]),n("p",[t._v("如上所示，首先使用tf.zeros得到一个张量，然后利用这个张量初始化得到一个变量，接着我们就可以打印这个变量，并且通"),n("code",[t._v("Object.prototype.toString.call(biases)")]),t._v("方法可以判断变量也是一个对象，接着，我们再生成一个张量，然后变量调用"),n("code",[t._v("assign")]),t._v("方法传入这个张量，就可以得到一个新的变量了，如下：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(151),alt:"An image"}})]),t._v(" "),n("p",[t._v("由此我们可以得出一个结论：变量由张量生成，且张量不可变而变量可变。")]),t._v(" "),n("h3",{attrs:{id:"模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#模型","aria-hidden":"true"}},[t._v("#")]),t._v(" 模型")]),t._v(" "),n("p",[t._v("在Tensorflow.js中，从概念上来说，一个模型就是一个给定一些输入将会产生特定的输出的函数。简单来说，一个模型就是一个函数，只是它完成了特定的任务。")]),t._v(" "),n("p",[t._v("在TensorFlow.js中有两种方式来创建模型，一种是通过操作（ops）来直接完成模型本身所做的工作，另外一种就是通过高级API "),n("code",[t._v("tf.model")]),t._v("来创建一个模型，显然第二种是更容易的。")]),t._v(" "),n("p",[t._v("我们先看第一种创建模型的方法：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// y = a * x ^ 2 + b * x + c")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// More on tf.tidy in the next section")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tidy")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" x "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" ax2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" a"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mul")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("square")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" bx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mul")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ax2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" a "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" b "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" result "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nresult"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("如上所示，我们定义的predict函数就是一个模型，对于给定的输入，我们就可以得到预测的输出。注意：所有的数字都需要经过tf.scalar()张量处理。")]),t._v(" "),n("p",[t._v("而第二种创建模型的方法就是用 TensorFlow.js 中的 tf.model 方法（这里的model并不是真正可以调用的方法，而是一个总称，比如实际上可以调用的是tf.sequential模型），这在深度学习中是非常流行的概念。 下面的代码就创建了 tf.sequential 模型：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sequential")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("simpleRNN")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    units"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    recurrentInitializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'GlorotNormal'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    inputShape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("80")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sgd")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("LEARNING_RATE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categoricalCrossentropy'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("h3",{attrs:{id:"内存管理"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#内存管理","aria-hidden":"true"}},[t._v("#")]),t._v(" 内存管理")]),t._v(" "),n("p",[t._v("因为TensorFlow.js使用了GPU来加速数学运算，因此当tensorflow处理张量和变量时就有必要来管理GPU内存。在TensorFlow.js中，我们可以通过dispose 和 tf.tidy这两种方法来管理内存。")]),t._v(" "),n("p",[n("strong",[t._v("dispose")])]),t._v(" "),n("p",[t._v("您可以在张量或变量上调用dispose来清除它并释放其GPU内存：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" x "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" x_squared "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("square")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dispose")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nx_squared"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dispose")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("strong",[t._v("tf.tidy")])]),t._v(" "),n("p",[t._v("进行大量的张量操作时使用dispose可能会很麻烦。TensorFlow.js提供了另一个函数tf.tidy，它对JavaScript中的常规范围起到类似的作用，不同的是它针对GPU支持的张量。")]),t._v(" "),n("p",[t._v("tf.tidy执行一个函数并清除所有创建的中间张量，释放它们的GPU内存。 它不清除内部函数的返回值。")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" average "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tidy")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" y "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tensor1d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" z "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("ones")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sub")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("z"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("square")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mean")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\naverage"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("使用tf.tidy将有助于防止应用程序中的内存泄漏。它也可以用来更谨慎地控制内存何时回收。")]),t._v(" "),n("p",[t._v("两个重要的注意事项")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("传递给tf.tidy的函数应该是同步的，并且不会返回Promise。我们建议在tf.tidy内不要有更新UI或在发出远程请求的代码。")])]),t._v(" "),n("li",[n("p",[t._v("tf.tidy不会清理变量。变量通常持续到机器学习模型的整个生命周期，因此TensorFlow.js不会清理它们，即使它们是在tidy中创建的。不过，您可以手动调用dispose处理它们。")])])]),t._v(" "),n("h2",{attrs:{id:"训练"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#训练","aria-hidden":"true"}},[t._v("#")]),t._v(" 训练")]),t._v(" "),n("h3",{attrs:{id:"拟合曲线"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#拟合曲线","aria-hidden":"true"}},[t._v("#")]),t._v(" 拟合曲线")]),t._v(" "),n("p",[t._v("这篇文章中，我们将使用TensorFlow.js来根据数据拟合曲线。即使用多项式产生数据然后再改变其中某些数据（点），然后我们会训练模型来找到用于产生这些数据的多项式的系数。简单的说，就是给一些在二维坐标中的散点图，然后我们建立一个系数未知的多项式，通过TensorFlow.js来训练模型，最终找到这些未知的系数，让这个多项式和散点图拟合。")]),t._v(" "),n("p",[n("strong",[t._v("先决条件")])]),t._v(" "),n("p",[t._v("本教程假定您熟悉核心概念中介绍的TensorFlow.js的基本构建块：张量，变量和操作。 我们建议在完成本教程之前先完成核心概念的学习。")]),t._v(" "),n("p",[n("strong",[t._v("运行代码")])]),t._v(" "),n("p",[t._v("这篇文章关注的是创建模型以及学习模型的系数，完整的代码在这里可以找到。为了在本地运行，如下所示：")]),t._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[t._v("$ "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("git")]),t._v(" clone https://github.com/tensorflow/tfjs-examples\n$ "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("cd")]),t._v(" tfjs-examples/polynomial-regression-core\n$ yarn\n$ yarn "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("watch")]),t._v("\n")])])]),n("p",[t._v("即首先将核心代码下载到本地，然后进入polynomial-regression-core（即多项式回归核心）部分，最后进行yarn安装并运行。")]),t._v(" "),n("p",[n("strong",[t._v("输入数据")])]),t._v(" "),n("p",[t._v("我们的数据集由x坐标和y坐标组成，当绘制在笛卡尔平面上时，其坐标如下所示：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(152),alt:"An image"}})]),t._v(" "),n("p",[t._v("该数据是由三次方程 "),n("code",[t._v("y = ax^{3}+ bx^{2} + cx + d")]),t._v(" 生成的。")]),t._v(" "),n("p",[t._v("我们的任务是学习这个函数的a，b，c和d系数以最好地拟合数据。 我们来看看如何使用TensorFlow.js操作来学习这些值。")]),t._v(" "),n("p",[n("strong",[t._v("第1步：设置变量")])]),t._v(" "),n("p",[t._v("首先，我们需要创建一些变量。即开始我们是不知道a、b、c、d的值的，所以先给他们一个随机数，入戏所示：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" a "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("variable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Math"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("random")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" b "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("variable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Math"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("random")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("variable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Math"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("random")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" d "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("variable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Math"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("random")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("strong",[t._v("第2步：建立模型")])]),t._v(" "),n("p",[t._v("我们可以通过TensorFlow.js中的链式调用操作来实现这个多项式方程 "),n("code",[t._v("y = ax3 + bx2 + cx + d")]),t._v("，下面的代码就创建了一个 predict 函数，这个函数将x作为输入，y作为输出：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// y = a * x ^ 3 + b * x ^ 2 + c * x + d")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("tidy")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" a"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mul")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("pow")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scalar")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// a * x^3")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mul")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("square")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// + b * x ^ 2")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mul")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// + c * x")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// + d")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("其中，在上一篇文章中，我们讲到tf.tify函数用来清除中间张量，其他的都很好理解。")]),t._v(" "),n("p",[t._v("接着，让我们把这个多项式函数的系数使用之前得到的随机数，可以看到，得到的图应该是这样：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(153),alt:"An image"}})]),t._v(" "),n("p",[t._v("因为开始时，我们使用的系数是随机数，所以这个函数和给定的数据匹配的非常差，而我们写的模型就是为了通过学习得到更精确的系数值。")]),t._v(" "),n("p",[n("strong",[t._v("第3步：训练模型")])]),t._v(" "),n("p",[t._v("最后一步就是要训练这个模型使得系数和这些散点更加匹配，而为了训练模型，我们需要定义下面的三样东西：")]),t._v(" "),n("ul",[n("li",[t._v("损失函数（loss function）：这个损失函数代表了给定多项式和数据的匹配程度。 损失函数值越小，那么这个多项式和数据就跟匹配。")]),t._v(" "),n("li",[t._v("优化器（optimizer）：这个优化器实现了一个算法，它会基于损失函数的输出来修正系数值。所以优化器的目的就是尽可能的减小损失函数的值。")]),t._v(" "),n("li",[t._v("训练迭代器（traing loop）：即它会不断地运行这个优化器来减少损失函数。")])]),t._v(" "),n("p",[t._v("所以，上面这三样东西的 关系就非常清楚了： 训练迭代器使得优化器不断运行，使得损失函数的值不断减小，以达到多项式和数据尽可能匹配的目的。这样，最终我们就可以得到a、b、c、d较为精确的值了。")]),t._v(" "),n("p",[n("strong",[t._v("定义损失函数")])]),t._v(" "),n("p",[t._v("这篇文章中，我们使用MSE（均方误差，mean squared error）作为我们的损失函数。MSE的计算非常简单，就是先根据给定的x得到实际的y值与预测得到的y值之差 的平方，然后在对这些差的平方求平均数即可。")]),t._v(" "),n("p",[n("img",{attrs:{src:a(154),alt:"An image"}})]),t._v(" "),n("p",[t._v("于是，我们可以这样定义MSE损失函数：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("loss")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 将labels（实际的值）进行抽象")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 然后获取平均数.")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" meanSquareError "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" predictions"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sub")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("square")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mean")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" meanSquareError"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("即这个损失函数返回的就是一个均方差，如果这个损失函数的值越小，显然数据和系数就拟合的越好。")]),t._v(" "),n("p",[n("strong",[t._v("定义优化器")])]),t._v(" "),n("p",[t._v("对于我们的优化器而言，我们选用 SGD （Stochastic Gradient Descent）优化器，即随机梯度下降。SGD的工作原理就是利用数据中任意的点的梯度以及使用它们的值来决定增加或者减少我们模型中系数的值。")]),t._v(" "),n("p",[t._v("TensorFlow.js提供了一个很方便的函数用来实现SGD，所以你不需要担心自己不会这些特别复杂的数学运算。 即 tf.train.sdg 将一个学习率（learning rate）作为输入，然后返回一个SGDOptimizer对象，它与优化损失函数的值是有关的。")]),t._v(" "),n("p",[t._v("在提高它的预测能力时，学习率（learning rate）会控制模型调整幅度将会有多大。低的学习率会使得学习过程运行的更慢一些（更多的训练迭代获得更符合数据的系数），而高的学习率将会加速学习过程但是将会导致最终的模型可能在正确值周围摇摆。简单的说，你既想要学的快，又想要学的好，这是不可能的。")]),t._v(" "),n("p",[t._v("下面的代码就创建了一个学习率为0.5的SGD优化器。")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" learningRate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sgd")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learningRate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("strong",[t._v("定义训练循环")])]),t._v(" "),n("p",[t._v("既然我们已经定义了损失函数和优化器，那么现在我们就可以创建一个训练迭代器了，它会不断地运行SGD优化器来使不断修正、完善模型的系数来减小损失（MSE）。下面就是我们创建的训练迭代器：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("train")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" numIterations "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("75")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" learningRate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sgd")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learningRate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("let")]),t._v(" iter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" iter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" numIterations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" iter"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("minimize")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" predsYs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("loss")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predsYs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("现在，让我们一步一步地仔细看看上面的代码。首先，我们定义了训练函数，并且以数据中x和y的值以及制定的迭代次数作为输入：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("train")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" numIterations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("...")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("接下来，我们定义了之前讨论过的学习率（learning rate）以及SGD优化器：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" learningRate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sgd")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learningRate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("最后，我们定义了一个for循环，这个循环会运行numIterations次训练。在每一次迭代中，我们都调用了optimizer优化器的minimize函数，这就是见证奇迹的地方：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("let")]),t._v(" iter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" iter "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" numIterations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" iter"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("minimize")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" predsYs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("loss")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predsYs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[n("code",[t._v("minimize")]),t._v(" 接受了一个函数作为参数，这个函数做了下面的两件事情：")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("首先它对所有的x值通过我们在之前定义的pridict函数预测了y值。")])]),t._v(" "),n("li",[n("p",[t._v("然后它通过我们之前定义的损失函数返回了这些预测的均方误差。")])])]),t._v(" "),n("p",[n("code",[t._v("minimize")]),t._v("函数之后会自动调整这些变量（即系数a、b、c、d）来使得损失函数更小。")]),t._v(" "),n("p",[t._v("在运行训练迭代器之后，a、b、c以及d就会是通过模型75次SGD迭代之后学习到的结果了。")]),t._v(" "),n("p",[t._v("查看结果！")]),t._v(" "),n("p",[t._v("一旦程序运行结束，我们就可以得到最终的a、b、c和d的结果了，然后使用它们来绘制曲线，如下所示：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(155),alt:"An image"}})]),t._v(" "),n("p",[t._v("这个结果已经比开始随机分配系数的结果拟合的好得多了！")]),t._v(" "),n("h3",{attrs:{id:"图片训练"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#图片训练","aria-hidden":"true"}},[t._v("#")]),t._v(" 图片训练")]),t._v(" "),n("p",[t._v("这篇文章中，我们将使用CNN构建一个Tensorflow.js模型来分辨手写的数字。首先，我们通过使之“查看”数以千计的数字图片以及他们对应的标识来训练分辨器。然后我们再通过此模型从未“见到”过的测试数据评估这个分辨器的精确度。")]),t._v(" "),n("p",[n("strong",[t._v("运行代码")])]),t._v(" "),n("p",[t._v("这篇文章的全部代码可以在仓库TensorFlow.js examples 中的tfjs-examples/mnist 下找到，你可以通过下面的方式clone下来然后运行这个demo：")]),t._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[t._v("$ "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("git")]),t._v(" clone https://github.com/tensorflow/tfjs-examples\n$ "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("cd")]),t._v(" tfjs-examples/mnist\n$ yarn\n$ yarn "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("watch")]),t._v("\n")])])]),n("p",[t._v("上面的这个目录完全是独立的，所以完全可以copy下来然后创建你个人的项目。")]),t._v(" "),n("p",[n("strong",[t._v("数据相关")])]),t._v(" "),n("p",[t._v("这里我们将会使用  MNIST  的手写数据，这些我们将要去分辨的手写数据如下所示：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(156),alt:"An image"}}),t._v(" "),n("img",{attrs:{src:a(157),alt:"An image"}}),t._v(" "),n("img",{attrs:{src:a(158),alt:"An image"}})]),t._v(" "),n("p",[t._v("为了预处理这些数据，我们已经写了 data.js， 这个文件包含了Minsdata类，而这个类可以帮助我们从MNIST的数据集中获取到任意的一些列的MNIST。")]),t._v(" "),n("p",[t._v("而MnistData这个类将全部的数据分割成了训练数据和测试数据。我们训练模型的时候，分辨器就会只观察训练数据。而当我们评价模型时，我们就仅仅使用测试数据，而这些测试数据是模型还没有看见到的，这样就可以来观察模型预测全新的数据了。")]),t._v(" "),n("p",[t._v("这个MnistData有两个共有方法：")]),t._v(" "),n("ol",[n("li",[n("p",[t._v("nextTrainBatch(batchSize)： 从训练数据中返回一批任意的图片以及他们的标识。")])]),t._v(" "),n("li",[n("p",[t._v("nextTestBatch(batchSize):  从测试数据中返回一批图片以及他们的标识。")])])]),t._v(" "),n("div",{staticClass:"warning custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("注意")]),t._v(" "),n("p",[t._v("当我们训练MNIST分辨器时，应当注意数据获取的任意性是非常重要的，这样模型预测才不会受到我们提供图片顺序的干扰。例如，如果我们每次给这个模型第一次都提供的是数字1，那么在训练期间，这个模型就会简单的预测第一个就是1（因为这样可以减小损失函数）。 而如果我们每次训练时都提供的是2，那么它也会简单切换为预测2并且永远不会预测1（同样的，也是因为这样可以减少损失函数）。如果每次都提供这样典型的、有代表性的数字，那么这个模型将永远也学不会做出一个精确的预测。")])]),t._v(" "),n("p",[n("strong",[t._v("创建模型")])]),t._v(" "),n("p",[t._v("在这一部分，我们将会创建一个卷积图片识别模型。为了这样做，我们使用了 "),n("code",[t._v("Sequential")]),t._v(" 模型（模型中最为简单的一个类型），在这个模型中，张量（tensors）可以连续的从一层传递到下一层中。")]),t._v(" "),n("p",[t._v("首先，我们需要使用 "),n("code",[t._v("tf.sequential")]),t._v(" 先初始化一个 "),n("code",[t._v("Sequential")]),t._v(" 模型：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sequential")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("既然我们已经创建了一个模型，那么我们就可以添加层了。")]),t._v(" "),n("p",[n("strong",[t._v("添加第一层")])]),t._v(" "),n("p",[t._v("我们要添加的第一层是一个2维的卷积层。卷积将过滤窗口掠过图片来学习空间上来说不会转变的变量（即图片中不同位置的模式或者物体将会被平等对待）。")]),t._v(" "),n("p",[t._v("我们可以通过 "),n("code",[t._v("tf.layers.conv2d")]),t._v(" 来创建一个2维的卷积层，这个卷积层可以接受一个配置对象来定义层的结构，如下所示：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("conv2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  inputShape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  kernelSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  filters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  strides"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  activation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  kernelInitializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'VarianceScaling'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("让我们拆分对象中的每个参数吧：")]),t._v(" "),n("ul",[n("li",[n("p",[n("code",[t._v("inputShape")]),t._v("：这个数据的形状将回流入模型的第一层。在这个示例中，我们的MNIST例子是28 x 28像素的黑白图片，这个关于图片的特定的格式即 "),n("code",[t._v("[row, column, depth]")]),t._v("，所以我们想要配置一个 "),n("code",[t._v("[28, 28, 1]")]),t._v("的形状，其中28行和28列是这个数字在每个维度上的像素数，且其深度为1，这是因为我们的图片只有1个颜色:")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("kernelSize")]),t._v("：划过卷积层过滤窗口的数量将会被应用到输入数据中去。这里，我们设置了 "),n("code",[t._v("kernalSize")]),t._v(" 的值为 "),n("code",[t._v("5")]),t._v("，也就是指定了一个5 x 5的卷积窗口。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("filters")]),t._v("：这个 "),n("code",[t._v("kernelSize")]),t._v(" 的过滤窗口的数量将会被应用到输入数据中，我们这里将8个过滤器应用到数据中。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("strides")]),t._v("： 即滑动窗口每一步的步长。比如每当过滤器移动过图片时将会由多少像素的变化。这里，我们指定其步长为1，这意味着每一步都是1像素的移动。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("activation")]),t._v("：这个activation函数将会在卷积完成之后被应用到数据上。在这个例子中，我们应用了relu 函数，这个函数在机器学习中是一个非常常见的激活函数。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("kernelInitializer")]),t._v(" ：这个方法对于训练动态的模型是非常重要的，他被用于任意地初始化模型的 weights。我们这里将不会深入细节来讲，但是 "),n("code",[t._v("VarianceScaling")]),t._v("（即这里用的）真的是一个初始化非常好的选择。")])])]),t._v(" "),n("p",[n("strong",[t._v("添加第二层")])]),t._v(" "),n("p",[t._v("让我们为这个模型添加第二层：一个最大的池化层（pooling layer），这个层中我们将通过 "),n("code",[t._v("tf.layers.maxPooling2d")]),t._v(" 来创建。这一层将会通过在每个滑动窗口中计算最大值来降频取样得到结果。")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("maxPooling2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  poolSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  strides"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("ul",[n("li",[n("p",[n("code",[t._v("poolSize")]),t._v("：这个滑动池窗口的数量将会被应用到输入的数据中。这里我们设置 "),n("code",[t._v("poolSize")]),t._v(" 为 "),n("code",[t._v("[2, 2]")]),t._v("，所以这就意味着池化层将会对输入数据应用2x2的窗口。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("strides")]),t._v("：这个池化层的步长大小。比如，当每次挪开输入数据时窗口需要移动多少像素。这里我们指定 "),n("code",[t._v("strides")]),t._v(" 为 "),n("code",[t._v("[2, 2]")]),t._v("，这就意味着过滤器将会以在水平方向和竖直方向上同时移动2个像素的方式来划过图片。")])])]),t._v(" "),n("div",{staticClass:"warning custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("注意")]),t._v(" "),n("p",[t._v("因为 "),n("code",[t._v("poolSize")]),t._v(" 和 "),n("code",[t._v("strides")]),t._v(" 都是2x2，所以池化层空口将会完全不会重叠。这也就意味着池化层将会把激活的大小从上一层减少一半。")])]),t._v(" "),n("p",[n("strong",[t._v("添加剩下的层")])]),t._v(" "),n("p",[t._v("重复使用层结构是神经网络中的常见模式。我们添加第二个卷积层到模型，并在其后添加池化层。请注意，在我们的第二个卷积层中，我们将滤波器数量从8增加到16。还要注意，我们没有指定 "),n("code",[t._v("inputShape")]),t._v("，因为它可以从前一层的输出形状中推断出来：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("conv2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  kernelSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  filters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  strides"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  activation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  kernelInitializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'VarianceScaling'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nmodel"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("maxPooling2d")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  poolSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  strides"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("接下来，我们添加一个 "),n("code",[t._v("flatten")]),t._v(" 层，将前一层的输出平铺到一个向量中：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("flatten")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("最后，让我们添加一个 "),n("code",[t._v("dense")]),t._v(" 层（也称为全连接层），它将执行最终的分类。 在dense层前先对卷积+池化层的输出执行flatten也是神经网络中的另一种常见模式：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("dense")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  units"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  kernelInitializer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'VarianceScaling'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  activation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("我们来分析传递给 "),n("code",[t._v("dense")]),t._v(" 层的参数。")]),t._v(" "),n("ul",[n("li",[n("p",[n("code",[t._v("units")]),t._v("：激活输出的数量。由于这是最后一层，我们正在做10个类别的分类任务（数字0-9），因此我们在这里使用10个units。 （有时units被称为神经元的数量，但我们会避免使用该术语。）")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("kernelInitializer")]),t._v("：我们将对dense层使用与卷积层相同的 "),n("code",[t._v("VarianceScaling")]),t._v(" 初始化策略。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("activation")]),t._v("：分类任务的最后一层的激活函数通常是 softmax。 Softmax将我们的10维输出向量归一化为概率分布，使得我们10个类中的每个都有一个概率值。")])])]),t._v(" "),n("p",[n("strong",[t._v("定义优化器")])]),t._v(" "),n("p",[t._v("对于我们的卷积神经网络模型，我们将使用学习率为0.15的随机梯度下降（SGD）优化器：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("LEARNING_RATE")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" optimizer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("sgd")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("LEARNING_RATE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("strong",[t._v("定义损失函数")])]),t._v(" "),n("p",[t._v("对于损失函数，我们将使用通常用于优化分类任务的交叉熵（ categoricalCrossentropy）。 categoricalCrossentropy度量模型的最后一层产生的概率分布与标签给出的概率分布之间的误差，这个分布在正确的类标签中为1（100％）。 例如，下面是数字7的标签和预测值：")]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",{staticStyle:{"text-align":"right"}},[t._v("class")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("1")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("2")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("3")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("4")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("5")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("6")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("7")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("8")]),t._v(" "),n("th",{staticStyle:{"text-align":"right"}},[t._v("9")])])]),t._v(" "),n("tbody",[n("tr",[n("td",{staticStyle:{"text-align":"right"}},[t._v("label")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("1")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v("0")])]),t._v(" "),n("tr",[n("td",{staticStyle:{"text-align":"right"}},[t._v("prediction")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".1")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".01")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".01")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".01")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".20")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".01")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".01")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".60")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".03")]),t._v(" "),n("td",{staticStyle:{"text-align":"right"}},[t._v(".02")])])])]),t._v(" "),n("p",[t._v("如果预测的结果是数字7的概率很高，那么categoricalCrossentropy会给出一个较低的损失值，而如果7的概率很低，那么categoricalCrossentropy的损失就会更高。在训练过程中，模型会更新它的内部参数以最小化在整个数据集上的categoricalCrossentropy。")]),t._v(" "),n("p",[n("strong",[t._v("定义评估指标")])]),t._v(" "),n("p",[t._v("对于我们的评估指标，我们将使用准确度，该准确度衡量所有预测中正确预测的百分比。")]),t._v(" "),n("p",[n("strong",[t._v("编译模型")])]),t._v(" "),n("p",[t._v("为了编译模型，我们传入一个由优化器，损失函数和一系列评估指标（这里只是'精度'）组成的配置对象：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" optimizer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categoricalCrossentropy'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  metrics"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("strong",[t._v("配置批量大小")])]),t._v(" "),n("p",[t._v("在开始训练之前，我们需要定义一些与batch size相关的参数：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TRAIN_BATCHES")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_BATCH_SIZE")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_ITERATION_FREQUENCY")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[n("em",[t._v("进一步了解分批量和批量大小")])]),t._v(" "),n("p",[t._v("为了充分利用GPU并行化计算的能力，我们希望将多个输入批量处理，并使用单个前馈网络调用将他们馈送到网络。")]),t._v(" "),n("p",[t._v("我们批量计算的另一个原因是，在优化过程中，我们只能在对多个样本中的梯度进行平均后更新内部参数（迈出一步）。这有助于我们避免因错误的样本（例如错误标记的数字）而朝错误的方向迈出了一步。")]),t._v(" "),n("p",[t._v("当批量输入数据时，我们引入秩D + 1的张量，其中D是单个输入的维数。")]),t._v(" "),n("p",[t._v("如前所述，我们MNIST数据集中单个图像的维度为[28,28,1]。当我们将BATCH_SIZE设置为64时，我们每次批量处理64个图像，这意味着我们的数据的实际形状是[64,28,28,1]（批量始终是最外层的维度）。")]),t._v(" "),n("div",{staticClass:"warning custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("注意")]),t._v(" "),n("p",[t._v("回想一下在我们的第一个conv2d配置中的inputShape没有指定批量大小（64）。 Config被写成批量大小不可知的，以便他们能够接受任意大小的批次。")])]),t._v(" "),n("p",[n("strong",[t._v("编写训练循环")])]),t._v(" "),n("p",[t._v("以下是训练循环的代码：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("let")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TRAIN_BATCHES")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" batch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("nextTrainBatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("let")]),t._v(" testBatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("let")]),t._v(" validationData"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_ITERATION_FREQUENCY")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    testBatch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("nextTestBatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    validationData "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n      testBatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reshape")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" testBatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" history "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reshape")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        batchSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validationData"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        epochs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" loss "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" accuracy "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("acc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("让我们分析代码。 首先，我们获取一批训练样本。 回想一下上面说的，我们利用GPU并行化批量处理样本，在对大量样本进行平均后才更新参数：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" batch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("nextTrainBatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("每5个step（TEST_ITERATION_FREQUENCY），我们构造一次validationData，这是一个包含一批来自MNIST测试集的图像及其相应标签这两个元素的数组，我们将使用这些数据来评估模型的准确性：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_ITERATION_FREQUENCY")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  testBatch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("nextTestBatch")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  validationData "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    testBatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reshape")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("TEST_BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    testBatch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[n("code",[t._v("model.fit")]),t._v(" 是模型训练和参数实际更新的地方。")]),t._v(" "),n("div",{staticClass:"warning custom-block"},[n("p",{staticClass:"custom-block-title"},[t._v("注意")]),t._v(" "),n("p",[t._v("在整个数据集上执行一次model.fit会导致将整个数据集上传到GPU，这可能会使应用程序死机。 为避免向GPU上传太多数据，我们建议在for循环中调用model.fit()，一次传递一批数据，如下所示：")])]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" history "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reshape")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("batchSize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("BATCH_SIZE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" validationData"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" validationData"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("我们再来分析一下这些参数：")]),t._v(" "),n("ul",[n("li",[n("p",[n("code",[t._v("x")]),t._v("：输入图像数据。请记住，我们分批量提供样本，因此我们必须告诉fit函数batch有多大。 MnistData.nextTrainBatch返回形状为[BATCH_SIZE，784]的图像 —— 所有的图像数据是长度为784（28 * 28）的一维向量。但是，我们的模型预期图像数据的形状为[BATCH_SIZE，28,28,1]，因此我们需要使用reshape函数。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("y")]),t._v("：我们的标签;每个图像的正确数字分类。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("batchSize")]),t._v("：每个训练batch中包含多少个图像。之前我们在这里设置的BATCH_SIZE是64。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("validationData")]),t._v("：每隔TEST_ITERATION_FREQUENCY（这里是5）个Batch，我们构建的验证集。该数据的形状为[TEST_BATCH_SIZE，28,28,1]。之前，我们设置了1000的TEST_BATCH_SIZE。我们的评估度量（准确度）将在此数据集上计算。")])]),t._v(" "),n("li",[n("p",[n("code",[t._v("epochs")]),t._v("：批量执行的训练次数。由于我们分批把数据馈送到fit函数，所以我们希望它每次仅从这个batch上进行训练。\n每次调用fit的时候，它会返回一个包含指标日志的对象，我们把它存储在history。我们提取每次训练迭代的损失和准确度，以便将它们绘制在图上：")])])]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" loss "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" accuracy "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("history"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("acc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("查看结果！")]),t._v(" "),n("p",[t._v("如果你运行完整的代码，你应该看到这样的输出：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(159),alt:"An image"}})]),t._v(" "),n("h3",{attrs:{id:"keras模型"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#keras模型","aria-hidden":"true"}},[t._v("#")]),t._v(" Keras模型")]),t._v(" "),n("p",[t._v("Keras模型（通常通过Python API创建）的存储格式可以有多种， 其中“整个模型”（即架构+权重+优化器状态都存在一个文件内）格式可以转换为 TensorFlow.js Layers格式，可以直接加载到TensorFlow.js中进行推理或进一步训练。")]),t._v(" "),n("p",[t._v("TensorFlow.js Layers格式是一个包含 "),n("code",[t._v("model.json")]),t._v(" 文件和一组二进制格式的权重文件分片的目录。 "),n("code",[t._v("model.json")]),t._v("文件包含模型拓扑结构（又名“体系结构”或“图”：层的描述以及它们如何连接）以及权重文件的清单。")]),t._v(" "),n("p",[n("strong",[t._v("要求")])]),t._v(" "),n("p",[t._v("运行模型格式转换程序需要Python环境;如果你想要一个独立的环境，你可以使用pipenv或virtualenv。要安装转换器，请使用这个命令 "),n("code",[t._v("pip install tensorflowjs")]),t._v("。")]),t._v(" "),n("p",[t._v("将Keras模型导入TensorFlow.js可以分为两个步骤。首先，将现有的Keras模型转换为TF.js Layers格式，然后将其加载到TensorFlow.js中。")]),t._v(" "),n("p",[n("strong",[t._v("第一步：将Keras模型转换为TF.js Layers格式")])]),t._v(" "),n("p",[t._v("Keras模型通常使用 "),n("code",[t._v("model.save(filepath)")]),t._v(" 保存，它生成一个包含模型拓扑结构和权重的HDF5（.h5）文件。要将这样的文件转换为TF.js Layer格式，请运行以下命令，其中 "),n("code",[t._v("path/to/my_model.h5")]),t._v(" 是Keras .h5源文件，"),n("code",[t._v("path/to/tfjs_target_dir")]),t._v(" 是TF.js文件的输出目录：")]),t._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[t._v("tensorflowjs_converter --input_format keras \\\n                       path/to/my_model.h5 \\\n                       path/to/tfjs_target_dir\n")])])]),n("p",[n("strong",[t._v("另一个方案：使用Python API直接导出为TF.js Layers格式")])]),t._v(" "),n("p",[t._v("如果您在Python中使用Keras模型，则可以将其直接导出为TensorFlow.js Layers格式，如下所示：")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflowjs "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tfjs\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("train")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# for example")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    tfjs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("converters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_keras_model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tfjs_target_dir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("strong",[t._v("第二步：将模型加载到TensorFlow.js中")])]),t._v(" "),n("p",[t._v("使用Web服务器来为您在步骤1中生成的转换模型文件提供服务。请注意，为了允许JavaScript获取文件，您可能需要配置服务器以允许跨源资源共享（CORS）。")]),t._v(" "),n("p",[t._v("然后通过提供model.json文件的URL将模型加载到TensorFlow.js中：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// JavaScript")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'@tensorflow/tfjs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("await")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("loadModel")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://foo.bar/tfjs_artifacts/model.json'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("现在该模型已准备好进行推理，评估或重新训练。例如，加载的模型可以立即用于预测：")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// JavaScript")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" example "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("fromPixels")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("webcamElement"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// for example")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" prediction "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("predict")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("example"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("TensorFlow.js Examples中的很多例子都采用此方法，使用已在Google云端存储上转换并托管的预训练模型。")]),t._v(" "),n("p",[t._v("请注意，您使用 "),n("code",[t._v("model.json")]),t._v(" 文件名来引用整个模型。"),n("code",[t._v("loadModel(...)")]),t._v(" 获取 "),n("code",[t._v("model.json")]),t._v("，然后发出额外的HTTP（S）请求以获取 "),n("code",[t._v("model.json")]),t._v(" 权重清单中引用的权重文件分片。这种方法允许所有这些文件被浏览器缓存（也可能通过互联网上的其他缓存服务器），因为 "),n("code",[t._v("model.json")]),t._v(" 和weight shard分别小于典型的缓存文件大小限制。因此，模型可能会在随后的场合更快加载。")]),t._v(" "),n("p",[n("strong",[t._v("支持的功能")])]),t._v(" "),n("p",[t._v("TensorFlow.js Layers目前仅支持使用标准Keras构造的Keras模型。使用了不受支持的操作或层（例如自定义层，Lambda层，自定义损失或自定义指标）的模型将无法自动导入，因为它们所依赖的Python代码无法正确地转换为JavaScript。")])])}],e=a(0),p=Object(e.a)({},function(){this.$createElement;this._self._c;return this._m(0)},n,!1,null,null,null);p.options.__file="README.md";s.default=p.exports}}]);